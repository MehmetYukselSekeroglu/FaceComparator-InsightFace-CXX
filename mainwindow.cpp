#include "mainwindow.h"
#include "./ui_mainwindow.h"

#include <QFileDialog>
#include <QMessageBox>
#include <QDebug>
#include <vector>
#include <algorithm>    // std::sort() i√ßin
#include <opencv2/imgproc.hpp>
#include <opencv2/highgui.hpp>
#include <opencv2/dnn.hpp> // NMS i√ßin
#include <QPixmap>
#include <QLabel>
#include <QResizeEvent>
#include <QIcon>



float MainWindow::calculateIOU(const cv::Rect& rect1, const cv::Rect& rect2)
{
    cv::Rect intersection = rect1 & rect2;
    float intersectionArea = (float)intersection.area();
    float unionArea = (float)(rect1.area() + rect2.area() - intersectionArea);
    if (unionArea < 1e-5) return 0.0f;
    return intersectionArea / unionArea;
}

void MainWindow::performNMS(std::vector<FaceProposal>& proposals, float iou_threshold, std::vector<FaceProposal>& final_faces)
{
    if (proposals.empty()) return;
    std::sort(proposals.begin(), proposals.end(), [](const FaceProposal& a, const FaceProposal& b) {
        return a.score > b.score;
    });
    std::vector<bool> suppressed(proposals.size(), false);
    for (size_t i = 0; i < proposals.size(); ++i)
    {
        if (suppressed[i]) continue;
        final_faces.push_back(proposals[i]);
        for (size_t j = i + 1; j < proposals.size(); ++j)
        {
            if (suppressed[j]) continue;
            float iou = calculateIOU(proposals[i].rect, proposals[j].rect);
            if (iou > iou_threshold)
            {
                suppressed[j] = true;
            }
        }
    }
}

QImage MainWindow::cvMatToQImage(const cv::Mat &mat)
{
    if(mat.type() == CV_8UC3)
        return QImage(mat.data, mat.cols, mat.rows, mat.step, QImage::Format_RGB888).rgbSwapped();
    else if(mat.type() == CV_8UC1)
        return QImage(mat.data, mat.cols, mat.rows, mat.step, QImage::Format_Grayscale8);
    else
        return QImage();
}


std::vector<FaceProposal> MainWindow::detectFacesInImage(const cv::Mat& image)
{
    std::vector<FaceProposal> final_faces;

    try {
        const int det_input_size = 640;
        const float original_width = (float)image.cols;
        const float original_height = (float)image.rows;

        // G√∂r√ºnt√ºy√º yeniden boyutlandƒ±r (orantƒ±yƒ± koruyarak)
        cv::Mat resized;
        float scale = std::min((float)det_input_size / original_width,
                               (float)det_input_size / original_height);
        int new_width = (int)(original_width * scale);
        int new_height = (int)(original_height * scale);

        cv::resize(image, resized, cv::Size(new_width, new_height));

        // Kenarlarƒ± siyah ile doldurarak 640x640 yap
        cv::Mat padded = cv::Mat::zeros(det_input_size, det_input_size, CV_8UC3);
        int x_offset = (det_input_size - new_width) / 2;
        int y_offset = (det_input_size - new_height) / 2;
        resized.copyTo(padded(cv::Rect(x_offset, y_offset, new_width, new_height)));

        // Blob hazƒ±rlama
        cv::Mat blob;
        cv::dnn::blobFromImage(padded, blob, 1.0/128.0,
                               cv::Size(det_input_size, det_input_size),
                               cv::Scalar(127.5, 127.5, 127.5),
                               true, false, CV_32F);

        std::vector<int64_t> inputShape = {1, 3, det_input_size, det_input_size};
        Ort::MemoryInfo memoryInfo = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);
        Ort::Value inputTensor = Ort::Value::CreateTensor<float>(memoryInfo,
                                                                 blob.ptr<float>(),
                                                                 blob.total(),
                                                                 inputShape.data(),
                                                                 inputShape.size());

        // Model √ßalƒ±≈ütƒ±r
        const char* inputNames[] = {"input.1"};
        const char* outputNames[] = {"448", "471", "494", "451", "474", "497", "454", "477", "500"};

        auto outputTensors = detector->Run(Ort::RunOptions{nullptr},
                                           inputNames, &inputTensor, 1,
                                           outputNames, 9);

        // Post-processing
        std::vector<FaceProposal> candidates;
        const float score_threshold = 0.5f;
        const std::vector<int> strides = {8, 16, 32};
        const std::vector<int> num_anchors_list = {12800, 3200, 800};
        const std::vector<int> feat_sizes = {80, 40, 20};

        for (int i = 0; i < strides.size(); i++)
        {
            int stride = strides[i];
            int feat_size = feat_sizes[i];
            int num_anchors = num_anchors_list[i];

            float* scores = outputTensors[i + 0].GetTensorMutableData<float>();
            float* bboxes = outputTensors[i + 3].GetTensorMutableData<float>();
            float* landmarks = outputTensors[i + 6].GetTensorMutableData<float>();

            for (int anchor_idx = 0; anchor_idx < num_anchors; anchor_idx++)
            {
                float score = scores[anchor_idx];
                if (score < score_threshold) continue;

                int grid_index = anchor_idx / 2;
                int grid_y = grid_index / feat_size;
                int grid_x = grid_index % feat_size;

                float center_x = (grid_x + 0.5f) * stride;
                float center_y = (grid_y + 0.5f) * stride;

                float dist_left = bboxes[anchor_idx * 4 + 0];
                float dist_top = bboxes[anchor_idx * 4 + 1];
                float dist_right = bboxes[anchor_idx * 4 + 2];
                float dist_bottom = bboxes[anchor_idx * 4 + 3];

                float x1 = center_x - dist_left * stride;
                float y1 = center_y - dist_top * stride;
                float x2 = center_x + dist_right * stride;
                float y2 = center_y + dist_bottom * stride;

                // Koordinat d√∂n√º≈ü√ºm√º
                float x1_unpad = (x1 - x_offset) / scale;
                float y1_unpad = (y1 - y_offset) / scale;
                float x2_unpad = (x2 - x_offset) / scale;
                float y2_unpad = (y2 - y_offset) / scale;

                if (x2_unpad <= x1_unpad || y2_unpad <= y1_unpad) {
                    continue;
                }

                x1_unpad = std::max(0.0f, std::min(x1_unpad, original_width - 1));
                y1_unpad = std::max(0.0f, std::min(y1_unpad, original_height - 1));
                x2_unpad = std::max(1.0f, std::min(x2_unpad, original_width));
                y2_unpad = std::max(1.0f, std::min(y2_unpad, original_height));

                // Landmarks
                std::vector<cv::Point2f> lms;
                for(int k = 0; k < 5; k++) {
                    float lx = center_x + landmarks[anchor_idx * 10 + k * 2 + 0] * stride;
                    float ly = center_y + landmarks[anchor_idx * 10 + k * 2 + 1] * stride;

                    float lx_unpad = (lx - x_offset) / scale;
                    float ly_unpad = (ly - y_offset) / scale;

                    lx_unpad = std::max(0.0f, std::min(lx_unpad, original_width - 1));
                    ly_unpad = std::max(0.0f, std::min(ly_unpad, original_height - 1));

                    lms.push_back(cv::Point2f(lx_unpad, ly_unpad));
                }

                FaceProposal proposal;
                proposal.score = score;
                proposal.rect = cv::Rect(
                    cv::Point((int)x1_unpad, (int)y1_unpad),
                    cv::Point((int)x2_unpad, (int)y2_unpad)
                    );

                if (proposal.rect.width < 20 || proposal.rect.height < 20) {
                    continue;
                }

                proposal.landmarks = lms;
                candidates.push_back(proposal);
            }
        }

        // NMS uygula
        performNMS(candidates, 0.3f, final_faces);

        // Y√ºksek g√ºvenilirlikteki y√ºzleri filtrele
        std::vector<FaceProposal> high_confidence_faces;
        for (const auto& face : final_faces) {
            if (face.score >= 0.6f) {
                high_confidence_faces.push_back(face);
            }
        }
        final_faces = high_confidence_faces;

    } catch (const std::exception& e) {
        qDebug() << "Face detection error:" << e.what();
    }

    return final_faces;
}
std::vector<float> MainWindow::getFaceEmbedding(const cv::Mat& image, const cv::Rect& faceRect)
{
    std::vector<float> embedding;

    try {
        // Y√ºz√º kƒ±rp ve 112x112 boyutuna getir
        cv::Mat faceCrop = image(faceRect).clone();

        // Debug: Kƒ±rpƒ±lan y√ºz√ºn boyutunu kontrol et
        if(faceCrop.empty()) {
            qDebug() << "Hata: Kƒ±rpƒ±lan y√ºz bo≈ü!";
            return embedding;
        }

        cv::resize(faceCrop, faceCrop, cv::Size(112, 112));
        cv::cvtColor(faceCrop, faceCrop, cv::COLOR_BGR2RGB);
        faceCrop.convertTo(faceCrop, CV_32FC3, 1.0/127.5, -1.0);

        // Embedding modeli i√ßin input hazƒ±rlama
        std::vector<int64_t> embedShape = {1, 3, 112, 112};
        std::vector<float> embedValues(3 * 112 * 112);

        // CHW formatƒ±na d√∂n√º≈üt√ºr
        for(int c = 0; c < 3; c++) {
            for(int row = 0; row < 112; row++) {
                for(int col = 0; col < 112; col++) {
                    embedValues[c * 112 * 112 + row * 112 + col] = faceCrop.at<cv::Vec3f>(row, col)[c];
                }
            }
        }

        Ort::MemoryInfo memoryInfo = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);
        Ort::Value embedInput = Ort::Value::CreateTensor<float>(memoryInfo,
                                                                embedValues.data(),
                                                                embedValues.size(),
                                                                embedShape.data(),
                                                                embedShape.size());

        // D√úZELTME: Output ismini "fc1" yerine "683" yap
        const char* embedInputNames[] = {"input.1"};
        const char* embedOutputNames[] = {"683"};  // <-- BURASI D√úZELDƒ∞

        auto embedOutput = embedder->Run(Ort::RunOptions{nullptr},
                                         embedInputNames, &embedInput, 1,
                                         embedOutputNames, 1);

        float* embedding_data = embedOutput.front().GetTensorMutableData<float>();
        size_t embedding_size = embedOutput.front().GetTensorTypeAndShapeInfo().GetElementCount();

        embedding = std::vector<float>(embedding_data, embedding_data + embedding_size);

        qDebug() << "Embedding boyutu:" << embedding.size(); // 512 olmalƒ±

    } catch (const std::exception& e) {
        qDebug() << "Embedding extraction error:" << e.what();
    }

    return embedding;
}
float MainWindow::cosineSimilarity(const std::vector<float>& vec1, const std::vector<float>& vec2)
{
    if (vec1.size() != vec2.size() || vec1.empty()) {
        return 0.0f;
    }

    float dot_product = 0.0f;
    float norm1 = 0.0f;
    float norm2 = 0.0f;

    for (size_t i = 0; i < vec1.size(); i++) {
        dot_product += vec1[i] * vec2[i];
        norm1 += vec1[i] * vec1[i];
        norm2 += vec2[i] * vec2[i];
    }

    if (norm1 < 1e-10 || norm2 < 1e-10) {
        return 0.0f;
    }

    return dot_product / (std::sqrt(norm1) * std::sqrt(norm2));
}

void MainWindow::drawFaceDetectionResult(cv::Mat& image, const std::vector<FaceProposal>& faces)
{
    for (size_t i = 0; i < faces.size(); i++) {
        const FaceProposal& face = faces[i];

        // Bounding box √ßiz
        cv::rectangle(image, face.rect, cv::Scalar(0, 255, 0), 2);

        // Landmark'larƒ± √ßiz
        for (const auto& lm : face.landmarks) {
            cv::circle(image, lm, 3, cv::Scalar(0, 0, 255), -1);
        }

        // Skor bilgisini yaz
        std::string score_text = "Score: " + std::to_string(face.score).substr(0, 4);
        cv::putText(image, score_text,
                    cv::Point(face.rect.x, face.rect.y - 10),
                    cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 255, 0), 1);

        // Y√ºz numarasƒ±nƒ± yaz
        std::string face_text = "Face " + std::to_string(i + 1);
        cv::putText(image, face_text,
                    cv::Point(face.rect.x, face.rect.y - 30),
                    cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(255, 0, 0), 1);
    }
}

void MainWindow::resizeEvent(QResizeEvent *event)
{
    QMainWindow::resizeEvent(event);

    // Member'daki pixmap'leri kullanarak labelleri yeniden √∂l√ßeklendir
    updateTab1Pixmap();
    updateTab2SourcePixmap();
    updateTab2TargetPixmap();
}
// Bu 3 fonksiyonu mainwindow.cpp dosyanƒ±za (√∂rn: resizeEvent'in √ºst√ºne) ekleyin

void MainWindow::updateTab1Pixmap()
{
    // Eƒüer pixmap bo≈üsa (√∂rn: program yeni ba≈üladƒ±ysa) default olanƒ± y√ºkle
    if (m_pixmapTab1.isNull()) {
        m_pixmapTab1 = QPixmap(":/images/resources/face_placeholder.png");
    }

    // Label boyutunu al ve pixmap'i √∂l√ßeklendir
    int width = ui->label_image_tab_1->width() > 10 ? ui->label_image_tab_1->width() : 100;
    int height = ui->label_image_tab_1->height() > 10 ? ui->label_image_tab_1->height() : 100;

    ui->label_image_tab_1->setPixmap(
        m_pixmapTab1.scaled(width, height, Qt::KeepAspectRatio, Qt::SmoothTransformation)
        );
}

void MainWindow::updateTab2SourcePixmap()
{
    if (m_pixmapTab2Source.isNull()) {
        m_pixmapTab2Source = QPixmap(":/images/resources/face_placeholder.png");
    }
    int width = ui->label_source_face_tab_2->width() > 10 ? ui->label_source_face_tab_2->width() : 100;
    int height = ui->label_source_face_tab_2->height() > 10 ? ui->label_source_face_tab_2->height() : 100;

    ui->label_source_face_tab_2->setPixmap(
        m_pixmapTab2Source.scaled(width, height, Qt::KeepAspectRatio, Qt::SmoothTransformation)
        );
}

void MainWindow::updateTab2TargetPixmap()
{
    if (m_pixmapTab2Target.isNull()) {
        m_pixmapTab2Target = QPixmap(":/images/resources/face_placeholder.png");
    }
    int width = ui->label_target_face_tab_2->width() > 10 ? ui->label_target_face_tab_2->width() : 100;
    int height = ui->label_target_face_tab_2->height() > 10 ? ui->label_target_face_tab_2->height() : 100;

    ui->label_target_face_tab_2->setPixmap(
        m_pixmapTab2Target.scaled(width, height, Qt::KeepAspectRatio, Qt::SmoothTransformation)
        );
}

MainWindow::MainWindow(QWidget *parent)
    : QMainWindow(parent)
    , ui(new Ui::MainWindow)
    , env(ORT_LOGGING_LEVEL_WARNING, "FaceRecognition")
    , detector(nullptr)
    , embedder(nullptr)
{
    ui->setupUi(this);
    setWindowIcon(QIcon(":/resources/app_icon.png"));

    // UI ba≈ülangƒ±√ß ayarlarƒ±
    updateTab1Pixmap();
    updateTab2SourcePixmap();
    updateTab2TargetPixmap();

    ui->label_image_tab_1->setAlignment(Qt::AlignCenter);
    ui->label_source_face_tab_2->setAlignment(Qt::AlignCenter);
    ui->label_target_face_tab_2->setAlignment(Qt::AlignCenter);

    setAboutText();
    this->setWindowTitle("FaceComparator | CXX Edition");
    ui->progressBar_sim_rate_tab_2->setValue(0);

    try
    {
        Ort::SessionOptions session_options;
        session_options.SetIntraOpNumThreads(1);
        session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);

        // Executable'ƒ±n bulunduƒüu dizinden model yollarƒ±nƒ± olu≈ütur
        QDir appDir(QCoreApplication::applicationDirPath());
        QString modelsPath = appDir.filePath("models");
        QString detModelPath = QDir(modelsPath).filePath("det_10g.onnx");
        QString embModelPath = QDir(modelsPath).filePath("w600k_r50.onnx");

        // Debug: Model yollarƒ±nƒ± kontrol et
        qDebug() << "Models directory:" << modelsPath;
        qDebug() << "Detection model path:" << detModelPath;
        qDebug() << "Embedding model path:" << embModelPath;

        // Dosyalarƒ±n var olup olmadƒ±ƒüƒ±nƒ± kontrol et
        if (!QFile::exists(detModelPath)) {
            QMessageBox::critical(this, "Model Hatasƒ±",
                                  QString("Tespit modeli bulunamadƒ±:\n%1\n\nL√ºtfen models klas√∂r√ºn√º kontrol edin.").arg(detModelPath));
            return;
        }

        if (!QFile::exists(embModelPath)) {
            QMessageBox::critical(this, "Model Hatasƒ±",
                                  QString("Embedding modeli bulunamadƒ±:\n%1\n\nL√ºtfen models klas√∂r√ºn√º kontrol edin.").arg(embModelPath));
            return;
        }

        // QString'i std::wstring'e √ßevir
        std::wstring model_detection_path = detModelPath.toStdWString();
        std::wstring model_embedding_path = embModelPath.toStdWString();

        detector = new Ort::Session(env, model_detection_path.c_str(), session_options);
        embedder = new Ort::Session(env, model_embedding_path.c_str(), session_options);

        // Debug i√ßin model bilgileri
        Ort::AllocatorWithDefaultOptions allocator;
        auto det_input_name = detector->GetInputNameAllocated(0, allocator);
        auto emb_input_name = embedder->GetInputNameAllocated(0, allocator);
        qDebug() << "Detector model input name:" << det_input_name.get();
        qDebug() << "Embedding model input name:" << emb_input_name.get();

        qDebug() << "Modeller ba≈üarƒ±yla y√ºklendi!";

    }
    catch (const Ort::Exception& e)
    {
        QMessageBox::critical(this, "Model Y√ºkleme Hatasƒ±",
                              QString("ONNX modelleri y√ºklenemedi.\n\nHata: %1").arg(e.what()));
        qApp->quit();
    }
    catch (const std::exception& e)
    {
        QMessageBox::critical(this, "Genel Hata",
                              QString("Beklenmedik bir hata olu≈ütu: %1").arg(e.what()));
        qApp->quit();
    }
}

MainWindow::~MainWindow()
{
    delete ui;
    // Pointer'larƒ± nullptr olarak initialize ettiƒüimiz i√ßin g√ºvenle delete edebiliriz
    if (detector) {
        delete detector;
        detector = nullptr;
    }
    if (embedder) {
        delete embedder;
        embedder = nullptr;
    }
}

void MainWindow::on_pushButton_selectFile_tab_1_clicked()
{
    QString filename = QFileDialog::getOpenFileName(this, "Open Image", "", "Images (*.png *.jpg *.jpeg *.bmp)");
    if(filename.isEmpty()) return;

    currentImage = cv::imread(filename.toStdString());
    if(currentImage.empty())
    {
        QMessageBox::warning(this, "Hata", "G√∂r√ºnt√º y√ºklenemedi!");
        return;
    }
    original_image_for_analysis = currentImage.clone();

    ui->label_image_tab_1->setPixmap(QPixmap::fromImage(cvMatToQImage(original_image_for_analysis))
                                         .scaled(ui->label_image_tab_1->size(),
                                                 Qt::KeepAspectRatio,
                                                 Qt::SmoothTransformation));
}
// MainWindow constructor'ƒ±nda veya ayrƒ± bir fonksiyonda
void MainWindow::setAboutText()
{
    QString aboutText =
        "<html>"
        "<head/>"
        "<body>"
        "<p align='center'><span style='font-size:16pt; font-weight:700; color:#3498db;'>FaceComparator</span></p>"
        "<p align='center'><span style='font-size:12pt; color:#ecf0f1;'>Y√ºz Tanƒ±ma ve Kar≈üƒ±la≈ütƒ±rma Sistemi</span></p>"
        "<p align='center'><span style='font-size:10pt; color:#bdc3c7;'>v2.0.1</span></p>"
        "<hr/>"
        "<p><span style='font-weight:600; color:#3498db;'>‚ú® √ñZELLƒ∞KLER:</span></p>"
        "<ul>"
        "<li>Y√ºksek Hassasiyetli Y√ºz Tespiti</li>"
        "<li>Detaylƒ± Analiz ve Raporlama</li>"
        "</ul>"
        "<p><span style='font-weight:600; color:#3498db;'>üõ† TEKNOLOJƒ∞LER:</span></p>"
        "<ul>"
        "<li>PyTorch &amp; ONNX Runtime</li>"
        "<li>InsightFace Buffalo_L Model</li>"
        "<li>OpenCV G√∂r√ºnt√º ƒ∞≈üleme</li>"
        "<li>Qt Modern Aray√ºz</li>"
        "</ul>"
        "<p><span style='font-weight:600; color:#3498db;'>‚öñÔ∏è Lƒ∞SANS:</span></p>"
        "<ul>"
        "<li>MIT Lisansƒ± ile lisanslanmƒ±≈ütƒ±r</li>"
        "<li>Ticari kullanƒ±mƒ± kesinlikle yasaktƒ±r</li>"
        "<li>Modellerin kendi lisans ko≈üullarƒ± ge√ßerlidir</li>"
        "</ul>"
        "<p><span style='font-weight:600; color:#3498db;'>üìû ƒ∞LETƒ∞≈ûƒ∞M:</span><br/>"
        "Telegram: @EyeOfWebSupport</p>"
        "<p><span style='font-weight:600; color:#3498db;'>üë®‚Äçüíª GELƒ∞≈ûTƒ∞Rƒ∞Cƒ∞:</span><br/>"
        "Mehmet Y√ºksel ≈ûekeroƒülu tarafƒ±ndan geli≈ütirilmi≈ütir</p>"
        "</body>"
        "</html>";

    ui->label_about->setText(aboutText);
}
void MainWindow::on_pushButton_detect_tab_1_clicked()
{
    try
    {
        if(original_image_for_analysis.empty())
        {
            QMessageBox::warning(this, "Hata", "G√∂r√ºnt√º y√ºklenmedi!");
            return;
        }

        cv::Mat displayImage = original_image_for_analysis.clone();
        const float original_width = (float)original_image_for_analysis.cols;
        const float original_height = (float)original_image_for_analysis.rows;

        // ---- Model Giri≈üi i√ßin G√∂r√ºnt√ºy√º Hazƒ±rla ----
        const int det_input_size = 640;

        // G√∂r√ºnt√ºy√º yeniden boyutlandƒ±r (orantƒ±yƒ± koruyarak)
        cv::Mat resized;
        float scale = std::min((float)det_input_size / original_width,
                               (float)det_input_size / original_height);
        int new_width = (int)(original_width * scale);
        int new_height = (int)(original_height * scale);

        cv::resize(original_image_for_analysis, resized, cv::Size(new_width, new_height));

        // Kenarlarƒ± siyah ile doldurarak 640x640 yap
        cv::Mat padded = cv::Mat::zeros(det_input_size, det_input_size, CV_8UC3);
        int x_offset = (det_input_size - new_width) / 2;
        int y_offset = (det_input_size - new_height) / 2;
        resized.copyTo(padded(cv::Rect(x_offset, y_offset, new_width, new_height)));

        // ---- BLOB HAZIRLAMA ----
        cv::Mat blob;
        cv::dnn::blobFromImage(padded, blob, 1.0/128.0,
                               cv::Size(det_input_size, det_input_size),
                               cv::Scalar(127.5, 127.5, 127.5),
                               true, false, CV_32F);

        std::vector<int64_t> inputShape = {1, 3, det_input_size, det_input_size};
        Ort::MemoryInfo memoryInfo = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);
        Ort::Value inputTensor = Ort::Value::CreateTensor<float>(memoryInfo,
                                                                 blob.ptr<float>(),
                                                                 blob.total(),
                                                                 inputShape.data(),
                                                                 inputShape.size());

        // ---- MODEL √áALI≈ûTIR ----
        const char* inputNames[] = {"input.1"};
        const char* outputNames[] = {"448", "471", "494", "451", "474", "497", "454", "477", "500"};

        auto outputTensors = detector->Run(Ort::RunOptions{nullptr},
                                           inputNames, &inputTensor, 1,
                                           outputNames, 9);

        // ---- D√úZELTƒ∞LMƒ∞≈û POST-PROCESSING ----
        std::vector<FaceProposal> candidates;
        const float score_threshold = 0.5f; // Threshold'u y√ºkselttik (daha az false positive)
        const std::vector<int> strides = {8, 16, 32};
        const std::vector<int> num_anchors_list = {12800, 3200, 800};
        const std::vector<int> feat_sizes = {80, 40, 20}; // 640/8, 640/16, 640/32

        for (int i = 0; i < strides.size(); i++)
        {
            int stride = strides[i];
            int feat_size = feat_sizes[i];
            int num_anchors = num_anchors_list[i];

            float* scores = outputTensors[i + 0].GetTensorMutableData<float>();
            float* bboxes = outputTensors[i + 3].GetTensorMutableData<float>();
            float* landmarks = outputTensors[i + 6].GetTensorMutableData<float>();

            for (int anchor_idx = 0; anchor_idx < num_anchors; anchor_idx++)
            {
                float score = scores[anchor_idx];
                if (score < score_threshold) continue;

                // **D√úZELTME: Doƒüru grid koordinat hesaplama**
                // Her grid h√ºcresi i√ßin 2 anchor var
                int grid_index = anchor_idx / 2;
                int grid_y = grid_index / feat_size;
                int grid_x = grid_index % feat_size;

                // Anchor merkez noktasƒ±
                float center_x = (grid_x + 0.5f) * stride;
                float center_y = (grid_y + 0.5f) * stride;

                // **D√úZELTME: Bounding box formatƒ±nƒ± d√ºzelt**
                // SCRFD formatƒ±: [distance_left, distance_top, distance_right, distance_bottom]
                float dist_left = bboxes[anchor_idx * 4 + 0];
                float dist_top = bboxes[anchor_idx * 4 + 1];
                float dist_right = bboxes[anchor_idx * 4 + 2];
                float dist_bottom = bboxes[anchor_idx * 4 + 3];

                // Bounding box koordinatlarƒ±nƒ± hesapla
                float x1 = center_x - dist_left * stride;
                float y1 = center_y - dist_top * stride;
                float x2 = center_x + dist_right * stride;
                float y2 = center_y + dist_bottom * stride;

                // **D√úZELTME: Koordinatlarƒ± orijinal g√∂r√ºnt√ºye d√∂n√º≈üt√ºr**
                // 1. √ñnce padding'i √ßƒ±kar
                float x1_unpad = x1 - x_offset;
                float y1_unpad = y1 - y_offset;
                float x2_unpad = x2 - x_offset;
                float y2_unpad = y2 - y_offset;

                // 2. Scale'i tersine √ßevir
                x1_unpad = x1_unpad / scale;
                y1_unpad = y1_unpad / scale;
                x2_unpad = x2_unpad / scale;
                y2_unpad = y2_unpad / scale;

                // **D√úZELTME: Ge√ßerli bounding box kontrol√º**
                if (x2_unpad <= x1_unpad || y2_unpad <= y1_unpad) {
                    continue; // Ge√ßersiz bounding box'ƒ± atla
                }

                // Sƒ±nƒ±rlarƒ± kontrol et
                x1_unpad = std::max(0.0f, std::min(x1_unpad, original_width - 1));
                y1_unpad = std::max(0.0f, std::min(y1_unpad, original_height - 1));
                x2_unpad = std::max(1.0f, std::min(x2_unpad, original_width)); // en az 1 pixel geni≈ülik
                y2_unpad = std::max(1.0f, std::min(y2_unpad, original_height)); // en az 1 pixel y√ºkseklik

                // Landmarks (5 points)
                std::vector<cv::Point2f> lms;
                for(int k = 0; k < 5; k++) {
                    float lx = center_x + landmarks[anchor_idx * 10 + k * 2 + 0] * stride;
                    float ly = center_y + landmarks[anchor_idx * 10 + k * 2 + 1] * stride;

                    // Landmark'larƒ± da aynƒ± ≈üekilde d√∂n√º≈üt√ºr
                    float lx_unpad = (lx - x_offset) / scale;
                    float ly_unpad = (ly - y_offset) / scale;

                    lx_unpad = std::max(0.0f, std::min(lx_unpad, original_width - 1));
                    ly_unpad = std::max(0.0f, std::min(ly_unpad, original_height - 1));

                    lms.push_back(cv::Point2f(lx_unpad, ly_unpad));
                }

                FaceProposal proposal;
                proposal.score = score;
                proposal.rect = cv::Rect(
                    cv::Point((int)x1_unpad, (int)y1_unpad),
                    cv::Point((int)x2_unpad, (int)y2_unpad)
                    );

                // **D√úZELTME: √áok k√º√ß√ºk bounding box'larƒ± filtrele**
                if (proposal.rect.width < 20 || proposal.rect.height < 20) {
                    continue; // √áok k√º√ß√ºk tespitleri atla
                }

                proposal.landmarks = lms;
                candidates.push_back(proposal);
            }
        }

        qDebug() << "Total candidates before NMS:" << candidates.size();

        // **D√úZELTME: Daha agresif NMS**
        std::vector<FaceProposal> final_faces;
        performNMS(candidates, 0.3f, final_faces); // IOU threshold'u d√º≈ü√ºrd√ºk

        // **D√úZELTME: Skor threshold'unu sonradan da uygula**
        std::vector<FaceProposal> high_confidence_faces;
        for (const auto& face : final_faces) {
            if (face.score >= 0.6f) { // Y√ºksek g√ºvenilirlikteki y√ºzleri al
                high_confidence_faces.push_back(face);
            }
        }

        ui->textBrowser_output_tab_1->append(QString("Bulunan Y√ºz Sayƒ±sƒ±: %1").arg(high_confidence_faces.size()));

        // Sonu√ßlarƒ± √ßiz
        for(size_t i = 0; i < high_confidence_faces.size(); i++)
        {
            FaceProposal& face = high_confidence_faces[i];

            // Debug: Bounding box bilgilerini yazdƒ±r
            qDebug() << "Face" << i << "rect:" << face.rect.x << face.rect.y << face.rect.width << "x" << face.rect.height << "score:" << face.score;

            cv::rectangle(displayImage, face.rect, cv::Scalar(0, 255, 0), 2);

            // Landmark'larƒ± √ßiz
            for(auto& lm : face.landmarks) {
                cv::circle(displayImage, lm, 3, cv::Scalar(0, 0, 255), -1);
            }

            // Skor bilgisini yaz
            cv::putText(displayImage,
                        QString("Score: %1").arg(face.score, 0, 'f', 2).toStdString(),
                        cv::Point(face.rect.x, face.rect.y - 10),
                        cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 255, 0), 1);

            ui->textBrowser_output_tab_1->append(
                QString("Face %1 (Score: %2) [%3,%4 %5x%6]")
                    .arg(i+1).arg(face.score, 0, 'f', 3)
                    .arg(face.rect.x).arg(face.rect.y)
                    .arg(face.rect.width).arg(face.rect.height)
                );
        }

        ui->label_image_tab_1->setPixmap(
            QPixmap::fromImage(cvMatToQImage(displayImage))
                .scaled(ui->label_image_tab_1->size(),
                        Qt::KeepAspectRatio,
                        Qt::SmoothTransformation)
            );

    }
    catch(const Ort::Exception &e)
    {
        QMessageBox::critical(this, "ONNX Hatasƒ±",
                              QString("Model √ßalƒ±≈ütƒ±rƒ±lƒ±rken bir hata olu≈ütu: %1").arg(e.what()));
        qDebug() << "ONNX Runtime Exception:" << e.what();
    }
    catch(const std::exception &e)
    {
        QMessageBox::critical(this, "Genel Hata",
                              QString("Beklenmedik bir hata olu≈ütu: %1").arg(e.what()));
        qDebug() << "Standard Exception:" << e.what();
    }
}

void MainWindow::on_pushButton_select_source_image_tab_2_clicked()
{
    QString filename = QFileDialog::getOpenFileName(this, "Kaynak Resmi Se√ß", "", "Images (*.png *.jpg *.jpeg *.bmp)");
    if(filename.isEmpty()) return;

    sourceImage_tab2 = cv::imread(filename.toStdString());
    if(sourceImage_tab2.empty())
    {
        QMessageBox::warning(this, "Hata", "Kaynak g√∂r√ºnt√º y√ºklenemedi!");
        return;
    }

    sourceImagePath_tab2 = filename;

    // G√∂r√ºnt√ºy√º label'a yerle≈ütir
    cv::Mat displayImage = sourceImage_tab2.clone();
    ui->label_source_face_tab_2->setPixmap(
        QPixmap::fromImage(cvMatToQImage(displayImage))
            .scaled(ui->label_source_face_tab_2->size(),
                    Qt::KeepAspectRatio,
                    Qt::SmoothTransformation)
        );

    ui->textBrowser_output_tab_2->append("Kaynak resim y√ºklendi: " + filename);
}


void MainWindow::on_pushButton_select_target_image_tab_2_clicked()
{
    QString filename = QFileDialog::getOpenFileName(this, "Hedef Resmi Se√ß", "", "Images (*.png *.jpg *.jpeg *.bmp)");
    if(filename.isEmpty()) return;

    targetImage_tab2 = cv::imread(filename.toStdString());
    if(targetImage_tab2.empty())
    {
        QMessageBox::warning(this, "Hata", "Hedef g√∂r√ºnt√º y√ºklenemedi!");
        return;
    }

    targetImagePath_tab2 = filename;

    // G√∂r√ºnt√ºy√º label'a yerle≈ütir
    cv::Mat displayImage = targetImage_tab2.clone();
    ui->label_target_face_tab_2->setPixmap(
        QPixmap::fromImage(cvMatToQImage(displayImage))
            .scaled(ui->label_target_face_tab_2->size(),
                    Qt::KeepAspectRatio,
                    Qt::SmoothTransformation)
        );

    ui->textBrowser_output_tab_2->append("Hedef resim y√ºklendi: " + filename);
}


void MainWindow::on_pushButton_compare_tab_2_clicked()
{
    if(sourceImage_tab2.empty() || targetImage_tab2.empty())
    {
        QMessageBox::warning(this, "Hata", "L√ºtfen √∂nce kaynak ve hedef resimleri se√ßin!");
        return;
    }

    compareFaces_tab2();
}


void MainWindow::on_pushButton_clear_tab_2_clicked()
{
    sourceImage_tab2 = cv::Mat();
    targetImage_tab2 = cv::Mat();
    sourceImagePath_tab2.clear();
    targetImagePath_tab2.clear();

    ui->label_source_face_tab_2->clear();
    ui->label_target_face_tab_2->clear();
    ui->textBrowser_output_tab_2->clear();
    ui->progressBar_sim_rate_tab_2->setValue(0);

    ui->label_source_face_tab_2->setText("Kaynak Resim");
    ui->label_target_face_tab_2->setText("Hedef Resim");
}

void MainWindow::compareFaces_tab2()
{
    ui->progressBar_sim_rate_tab_2->setValue(0);
    ui->textBrowser_output_tab_2->clear();
    ui->textBrowser_output_tab_2->append("=== Y√úZ KAR≈ûILA≈ûTIRMA BA≈ûLATILDI ===");

    try {
        // 1. Adƒ±m: Resimleri kontrol et
        ui->textBrowser_output_tab_2->append("\n1. Adƒ±m: Resimler kontrol ediliyor...");
        if (sourceImage_tab2.empty() || targetImage_tab2.empty()) {
            ui->textBrowser_output_tab_2->append("Hata: Resimler y√ºklenemedi!");
            return;
        }
        ui->progressBar_sim_rate_tab_2->setValue(10);

        // 2. Adƒ±m: Kaynak resimde y√ºz tespiti
        ui->textBrowser_output_tab_2->append("\n2. Adƒ±m: Kaynak resimde y√ºz tespiti...");
        std::vector<FaceProposal> faces1 = detectFacesInImage(sourceImage_tab2);
        if (faces1.empty()) {
            ui->textBrowser_output_tab_2->append("Hata: Kaynak resimde y√ºz bulunamadƒ±!");
            return;
        }
        ui->textBrowser_output_tab_2->append(QString("   - %1 y√ºz bulundu").arg(faces1.size()));
        ui->progressBar_sim_rate_tab_2->setValue(30);

        // 3. Adƒ±m: Hedef resimde y√ºz tespiti
        ui->textBrowser_output_tab_2->append("\n3. Adƒ±m: Hedef resimde y√ºz tespiti...");
        std::vector<FaceProposal> faces2 = detectFacesInImage(targetImage_tab2);
        if (faces2.empty()) {
            ui->textBrowser_output_tab_2->append("Hata: Hedef resimde y√ºz bulunamadƒ±!");
            return;
        }
        ui->textBrowser_output_tab_2->append(QString("   - %1 y√ºz bulundu").arg(faces2.size()));
        ui->progressBar_sim_rate_tab_2->setValue(50);

        // 4. Adƒ±m: G√∂mme vekt√∂rlerini √ßƒ±kar
        ui->textBrowser_output_tab_2->append("\n4. Adƒ±m: Y√ºz √∂zellik vekt√∂rleri √ßƒ±karƒ±lƒ±yor...");
        std::vector<std::vector<float>> embeddings1, embeddings2;

        for (size_t i = 0; i < faces1.size(); i++) {
            ui->textBrowser_output_tab_2->append(QString("   - Kaynak %1. y√ºz i≈üleniyor...").arg(i + 1));
            std::vector<float> embedding = getFaceEmbedding(sourceImage_tab2, faces1[i].rect);
            if (!embedding.empty()) {
                embeddings1.push_back(embedding);
            }
            ui->progressBar_sim_rate_tab_2->setValue(50 + (i * 10 / faces1.size()));
        }

        for (size_t i = 0; i < faces2.size(); i++) {
            ui->textBrowser_output_tab_2->append(QString("   - Hedef %1. y√ºz i≈üleniyor...").arg(i + 1));
            std::vector<float> embedding = getFaceEmbedding(targetImage_tab2, faces2[i].rect);
            if (!embedding.empty()) {
                embeddings2.push_back(embedding);
            }
            ui->progressBar_sim_rate_tab_2->setValue(60 + (i * 10 / faces2.size()));
        }

        if (embeddings1.empty() || embeddings2.empty()) {
            ui->textBrowser_output_tab_2->append("Hata: Y√ºz √∂zellik vekt√∂rleri √ßƒ±karƒ±lamadƒ±!");
            return;
        }
        ui->progressBar_sim_rate_tab_2->setValue(70);

        // 5. Adƒ±m: Benzerlik hesapla
        ui->textBrowser_output_tab_2->append("\n5. Adƒ±m: Benzerlik hesaplanƒ±yor...");

        float max_similarity = 0.0f;
        int best_pair_i = -1, best_pair_j = -1;
        std::vector<std::vector<float>> similarity_matrix(embeddings1.size(),
                                                          std::vector<float>(embeddings2.size(), 0.0f));

        for (size_t i = 0; i < embeddings1.size(); i++) {
            for (size_t j = 0; j < embeddings2.size(); j++) {
                float similarity = cosineSimilarity(embeddings1[i], embeddings2[j]);
                similarity_matrix[i][j] = similarity;

                if (similarity > max_similarity) {
                    max_similarity = similarity;
                    best_pair_i = i;
                    best_pair_j = j;
                }
            }
            ui->progressBar_sim_rate_tab_2->setValue(70 + (i * 20 / embeddings1.size()));
        }

        ui->progressBar_sim_rate_tab_2->setValue(90);

        // 6. Adƒ±m: Sonu√ßlarƒ± g√∂ster
        ui->textBrowser_output_tab_2->append("\n=== SONU√áLAR ===");

        // Progress bar'ƒ± benzerlik y√ºzdesine g√∂re ayarla
        int similarity_percentage = (int)(max_similarity * 100);
        ui->progressBar_sim_rate_tab_2->setValue(similarity_percentage);

        ui->textBrowser_output_tab_2->append(QString("En y√ºksek benzerlik: %1%").arg(similarity_percentage));

        // Benzerlik deƒüerlendirmesi
        QString similarity_comment;

        if (similarity_percentage >= 45) {
            similarity_comment = "%45 √úzeri Benzerlik Aynƒ± Ki≈üi Olma Olasƒ±lƒ±ƒüƒ± √áok Y√ºksek.";
        }else {
            similarity_comment = "Aynƒ± Ki≈üi Olma Olasƒ±lƒ±ƒüƒ± D√º≈ü√ºk";
        }


        ui->textBrowser_output_tab_2->append(similarity_comment);

        if (best_pair_i >= 0 && best_pair_j >= 0) {
            ui->textBrowser_output_tab_2->append(QString("\nE≈üle≈üen y√ºzler:"));
            ui->textBrowser_output_tab_2->append(QString("   - Kaynak resim %1. y√ºz").arg(best_pair_i + 1));
            ui->textBrowser_output_tab_2->append(QString("   - Hedef resim %1. y√ºz").arg(best_pair_j + 1));
            ui->textBrowser_output_tab_2->append(QString("   - Kaynak y√ºz g√ºven skoru: %1").arg(faces1[best_pair_i].score, 0, 'f', 3));
            ui->textBrowser_output_tab_2->append(QString("   - Hedef y√ºz g√ºven skoru: %1").arg(faces2[best_pair_j].score, 0, 'f', 3));
        }

        // Benzerlik matrisini g√∂ster
        if (embeddings1.size() > 1 || embeddings2.size() > 1) {
            ui->textBrowser_output_tab_2->append("\nBenzerlik Matrisi:");
            for (size_t i = 0; i < similarity_matrix.size(); i++) {
                QString row_text = QString("Kaynak %1: ").arg(i + 1);
                for (size_t j = 0; j < similarity_matrix[i].size(); j++) {
                    row_text += QString("Hedef%1:%2% ").arg(j + 1).arg((int)(similarity_matrix[i][j] * 100));
                }
                ui->textBrowser_output_tab_2->append(row_text);
            }
        }

        // 7. Adƒ±m: G√∂rsel sonu√ßlarƒ± hazƒ±rla ve g√∂ster
        cv::Mat result1 = sourceImage_tab2.clone();
        cv::Mat result2 = targetImage_tab2.clone();

        drawFaceDetectionResult(result1, faces1);
        drawFaceDetectionResult(result2, faces2);

        // En iyi e≈üle≈ümeyi vurgula
        if (best_pair_i >= 0 && best_pair_i < faces1.size()) {
            cv::rectangle(result1, faces1[best_pair_i].rect, cv::Scalar(255, 0, 0), 3);
            cv::putText(result1, "EN IYI ESLESME",
                        cv::Point(faces1[best_pair_i].rect.x, faces1[best_pair_i].rect.y - 50),
                        cv::FONT_HERSHEY_SIMPLEX, 0.7, cv::Scalar(255, 0, 0), 2);
        }

        if (best_pair_j >= 0 && best_pair_j < faces2.size()) {
            cv::rectangle(result2, faces2[best_pair_j].rect, cv::Scalar(255, 0, 0), 3);
            cv::putText(result2, "EN IYI ESLESME",
                        cv::Point(faces2[best_pair_j].rect.x, faces2[best_pair_j].rect.y - 50),
                        cv::FONT_HERSHEY_SIMPLEX, 0.7, cv::Scalar(255, 0, 0), 2);
        }

        // Label'lara i≈ülenmi≈ü resimleri yerle≈ütir
        ui->label_source_face_tab_2->setPixmap(
            QPixmap::fromImage(cvMatToQImage(result1))
                .scaled(ui->label_source_face_tab_2->size(),
                        Qt::KeepAspectRatio,
                        Qt::SmoothTransformation)
            );

        ui->label_target_face_tab_2->setPixmap(
            QPixmap::fromImage(cvMatToQImage(result2))
                .scaled(ui->label_target_face_tab_2->size(),
                        Qt::KeepAspectRatio,
                        Qt::SmoothTransformation)
            );

        ui->progressBar_sim_rate_tab_2->setValue(similarity_percentage);
        ui->textBrowser_output_tab_2->append("\n‚úÖ ƒ∞≈ülem tamamlandƒ±!");

    } catch (const std::exception& e) {
        ui->textBrowser_output_tab_2->append(QString("Hata: %1").arg(e.what()));
        ui->progressBar_sim_rate_tab_2->setValue(0);
    }
}
